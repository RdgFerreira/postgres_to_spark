services:
  db: # Service name
    image: postgis/postgis:17-3.5
    container_name: postgres_17
    restart: always
    env_file:
      - db.env
    ports:
      - "5432:5432"  # Maps host port 5432 to container port 5432
    volumes:
      - postgres_data:/var/lib/postgresql/data # Persists data outside the container
    networks:
      - my_data_flow_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d my_database"]
      interval: 1m
      timeout: 30s
      retries: 5
      start_period: 30s

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin
    restart: always
    depends_on:
      db:  # Ensures the database starts before pgAdmin
        condition: service_healthy
    env_file:
      - pg.env
    ports:
      - "8080:80"
    volumes:
      - pgadmin_data:/var/lib/pgadmin  # Persists pgAdmin data
    networks:
      - my_data_flow_network

  spark-master:
    image: postgres_to_spark-spark-jupyter
    container_name: spark_master
    command: ["sh", "-c", "/opt/spark/sbin/start-master.sh && tail -f /opt/spark/logs/*"]
    restart: always
    depends_on:
      - db
    environment:
      - SPARK_MODE=master
      - SPARK_NO_DAEMONIZE=true
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8082
    ports:
      - "7077:7077"  # Spark master port
      - "8082:8082"  # Spark master web UI port
    volumes:
      - ./scripts:/opt/spark/scripts
      - ./data:/opt/spark/data
      - spark-logs:/opt/spark/spark-events
      - ./jobs:/app
    networks:
      - my_data_flow_network

  spark-history-server:
    image: postgres_to_spark-spark-jupyter
    container_name: spark_history_server
    command: ["sh", "-c", "/opt/spark/sbin/start-history-server.sh && tail -f /opt/spark/logs/*"]
    restart: always
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=history-server
      - SPARK_NO_DAEMONIZE=true
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=/opt/spark/spark-events
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_HISTORY_WEBUI_PORT=18080
    ports:
      - "18080:18080"  # Spark history server web UI port
    volumes:
      - ./scripts:/opt/spark/scripts
      - ./data:/opt/spark/data
      - spark-logs:/opt/spark/spark-events
    networks:
      - my_data_flow_network

  spark-worker:
    image: postgres_to_spark-spark-jupyter
    container_name: spark_worker
    command: ["sh", "-c", "/opt/spark/sbin/start-worker.sh spark://spark-master:7077 && tail -f /opt/spark/logs/*"]
    restart: always
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_NO_DAEMONIZE=true
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_WORKER_WEBUI_PORT=8081
    ports:
      - "8081:8081"  # Spark worker web UI port
    volumes:
      - ./scripts:/opt/spark/scripts
      - ./data:/opt/spark/data
      - spark-logs:/opt/spark/spark-events
    networks:
      - my_data_flow_network

  spark-jupyter:
    build: .
    container_name: spark_jupyter
    command: ["jupyter", "lab", "--ip=0.0.0.0", "--port=8888", "--no-browser", "--allow-root"]
    restart: always
    depends_on:
      - spark-master
    env_file:
      - jupyter.env
    ports:
      - "8888:8888"
    volumes:
      # Mounts local notebooks (folder "jobs" on host is mapped to /app in both spark-master and spark-jupyter containers)
      - ./jobs:/app
    networks:
      - my_data_flow_network

  scheduler:
    build:
      context: ./dags
    container_name: scheduler
    command: bash -c "airflow db migrate && airflow scheduler"
    depends_on:
      - db
    env_file:
      - dags/.env
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    networks:
      - my_data_flow_network

  api-server:
    build:
      context: ./dags
    container_name: api-server
    command: bash -c "airflow api-server"
    env_file:
      - dags/.env
    restart: always
    depends_on:
      - scheduler
    ports:
      - "8086:8080"  # Maps host port 8086 to container port 8080 (Airflow API Server)
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    networks:
      - my_data_flow_network


volumes:
  postgres_data:
    driver: local # Local drive  
  pgadmin_data:
    driver: local
  spark-logs:
    driver: local

networks:
  my_data_flow_network: # Private network for container communication
    driver: bridge # Isolated Network on Host